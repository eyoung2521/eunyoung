{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "CNN"
      ],
      "metadata": {
        "id": "fhiYhPfdHaoo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-NTX1dGSHSi5"
      },
      "outputs": [],
      "source": [
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.applications import VGG16\n",
        "\n",
        "# 데이터셋 다운로드 오류 (링크 깨짐 해결)\n",
        "setattr(tfds.image_classification.cats_vs_dogs, '_URL',\"https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_5340.zip\")\n",
        "\n",
        "dataset_name = 'cats_vs_dogs'\n",
        "\n",
        "\n",
        "# 처음 80%의 데이터만 사용\n",
        "train_dataset = tfds.load(name=dataset_name, split='train[:80%]')\n",
        "\n",
        "# 최근 20%의 데이터만 사용\n",
        "valid_dataset = tfds.load(name=dataset_name, split='train[80%:]')\n",
        "\n",
        "def preprocess(data):\n",
        "    # x, y 데이터를 정의\n",
        "    x = data['image']\n",
        "    y = data['label']\n",
        "    # image 정규화(Normalization)\n",
        "    x = x / 255\n",
        "    # 사이즈를 (224, 224)로 변환\n",
        "    x = tf.image.resize(x, size=(224, 224))\n",
        "    # x, y  데이터를 return \n",
        "    return x, y\n",
        "\n",
        "def solution_model():\n",
        "    train_data = train_dataset.map(preprocess).batch(32)\n",
        "    valid_data = valid_dataset.map(preprocess).batch(32)\n",
        "\n",
        "    transfer_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "    transfer_model.trainable=False\n",
        "\n",
        "\n",
        "    model = Sequential([\n",
        "        transfer_model, #trainsfer learning 부분\n",
        "        Dropout(0.5),\n",
        "        Flatten(),\n",
        "        Dense(256, activation='relu'),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dense(2, activation='softmax'),])\n",
        "    \n",
        "\n",
        "    model.compile(loss='sparse_categorical_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics=['acc'])\n",
        "\n",
        "    checkpoint_path = \"tmp_checkpoint.ckpt\"\n",
        "\n",
        "    checkpoint = ModelCheckpoint(checkpoint_path,\n",
        "                                 save_weights_only=True,\n",
        "                                 save_best_only=True,\n",
        "                                 monitor='val_loss',\n",
        "                                 verbose=1)\n",
        "\n",
        "    model.fit(train_data,\n",
        "              validation_data=valid_data,\n",
        "              epochs=15,\n",
        "              verbose=1,\n",
        "              callbacks=[checkpoint],\n",
        "              )\n",
        "\n",
        "    model.load_weights(checkpoint_path)\n",
        "\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    model = solution_model()\n",
        "    model.save(\"TF3-cats-vs-dogs_1.h5\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib\n",
        "import zipfile\n",
        "\n",
        "import tensorflow as tf\n",
        "from keras_preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Dropout,Flatten, Dense, Conv2D, MaxPooling2D,Reshape\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "\n",
        "def download_and_extract_data():\n",
        "    url = 'https://storage.googleapis.com/download.tensorflow.org/data/certificate/germantrafficsigns.zip'\n",
        "    urllib.request.urlretrieve(url, 'germantrafficsigns.zip')\n",
        "    with zipfile.ZipFile('germantrafficsigns.zip', 'r') as zip_ref:\n",
        "        zip_ref.extractall()\n",
        "\n",
        "def preprocess(image, label):\n",
        "    image = image/255\n",
        "    return image, label\n",
        "\n",
        "\n",
        "def solution_model():\n",
        "    download_and_extract_data()\n",
        "    BATCH_SIZE = 32\n",
        "    IMG_SIZE = 30\n",
        "\n",
        "    train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "        directory='train/',\n",
        "        label_mode='categorical',\n",
        "        image_size=(IMG_SIZE,IMG_SIZE),\n",
        "        batch_size = BATCH_SIZE)\n",
        "\n",
        "    val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "        directory='validation/',\n",
        "        label_mode='categorical',\n",
        "        image_size=(IMG_SIZE,IMG_SIZE),\n",
        "        batch_size = BATCH_SIZE)\n",
        "\n",
        "    train_ds = train_ds.map(\n",
        "        preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE).prefetch(\n",
        "        tf.data.experimental.AUTOTUNE)\n",
        "    val_ds = val_ds.map(\n",
        "        preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "    # Code to define the model\n",
        "    model = tf.keras.models.Sequential([\n",
        "        Conv2D(32,(3,3),activation='relu',input_shape = (30,30,3)),\n",
        "        MaxPooling2D(2,2),\n",
        "        Conv2D(64,(3,3),activation='relu'),\n",
        "        MaxPooling2D(2,2),\n",
        "        Conv2D(64,(3,3),activation='relu'),\n",
        "        MaxPooling2D(2,2), # 이미지 자체가 너무 작기때문에 맥스풀링 너무 많이 x\n",
        "        Flatten(),\n",
        "        Dropout(0.25),\n",
        "        Dense(128,activation='relu'),\n",
        "        tf.keras.layers.Dense(43, activation=tf.nn.softmax)\n",
        "    ])\n",
        "\n",
        "    model.compile( optimizer = 'adam', loss = 'categorical_crossentropy',metrics = ['acc']\n",
        "    )\n",
        "\n",
        "    checkpoint_path = 'tmp_checkpoint.ckpt'\n",
        "    checkpoint = ModelCheckpoint(filepath=checkpoint_path,\n",
        "                                 save_weights_only=True,\n",
        "                                 save_best_only = True,\n",
        "                                 monitor = 'val_loss',\n",
        "                                 verbose=1)\n",
        "\n",
        "    model.fit(\n",
        "        train_ds,\n",
        "        validation_data = val_ds,\n",
        "        epochs=10,\n",
        "        callbacks=[checkpoint]\n",
        "    )\n",
        "    model.load_weights(checkpoint_path)\n",
        "    \n",
        "\n",
        "    return model\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    model = solution_model()\n",
        "    model.save(\"mymodel.h5\")"
      ],
      "metadata": {
        "id": "qFWnfbHvH4Q-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "fashion mnist 사용"
      ],
      "metadata": {
        "id": "mzAGPiAEHdka"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "\n",
        "def solution_model():\n",
        "    fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "    \n",
        "\n",
        "    (x_train,y_train),(x_valid,y_valid) = fashion_mnist.load_data()\n",
        "\n",
        "    x_train = x_train/255\n",
        "    x_valid = x_valid/255\n",
        "\n",
        "\n",
        "\n",
        "    model = Sequential([\n",
        "        Flatten(),\n",
        "        Dropout(0.25),\n",
        "        Dense(1024, activation='relu'),\n",
        "        Dense(512, activation='relu'),\n",
        "        Dense(256, activation='relu'),\n",
        "        Dense(256, activation='relu'),\n",
        "        Dropout(0.25),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dense(32, activation = 'relu'),\n",
        "    # Classification을 위한 Softmax \n",
        "        Dense(10, activation='softmax'),\n",
        "        ])\n",
        "    \n",
        "    model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc'])\n",
        "\n",
        "    checkpoint_path = \"my_checkpoint.ckpt\"\n",
        "    checkpoint = ModelCheckpoint(filepath=checkpoint_path, \n",
        "                                 save_weights_only=True, \n",
        "                                save_best_only=True, \n",
        "                                monitor='val_loss', \n",
        "                                verbose=1)\n",
        "    \n",
        "    model.fit(x_train, y_train,\n",
        "                    validation_data=(x_valid, y_valid),\n",
        "                    epochs=20,\n",
        "                    callbacks=[checkpoint],\n",
        "                   )\n",
        "    \n",
        "\n",
        "    model.load_weights(checkpoint_path)\n",
        "\n",
        "\n",
        "    \n",
        "    return model\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    model = solution_model()\n",
        "    model.save(\"TF2-fashion-mnist_7.h5\")"
      ],
      "metadata": {
        "id": "z6_B_c3UHopw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "자연어 처리 모델"
      ],
      "metadata": {
        "id": "bNP5NiwFHvLd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import json\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import urllib\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional, Flatten\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "def solution_model():\n",
        "    url = 'https://storage.googleapis.com/download.tensorflow.org/data/sarcasm.json'\n",
        "    urllib.request.urlretrieve(url, 'sarcasm.json')\n",
        "\n",
        "    with open('sarcasm.json') as f:\n",
        "        datas = json.load(f)\n",
        "        \n",
        "    vocab_size = 1000\n",
        "    embedding_dim = 16\n",
        "    max_length = 120\n",
        "    trunc_type='post'\n",
        "    padding_type='post'\n",
        "    oov_tok = \"<OOV>\"\n",
        "    training_size = 20000\n",
        "\n",
        "\n",
        "    sentences = []\n",
        "    labels = []\n",
        "\n",
        "\n",
        "    for data in datas:\n",
        "      sentences.append(data['headline'])\n",
        "      labels.append(data['is_sarcastic'])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    train_sentences = sentences[:training_size]\n",
        "    train_labels = labels[:training_size]\n",
        "\n",
        "    validation_sentences = sentences[training_size:]\n",
        "    validation_labels = labels[training_size:]\n",
        "\n",
        "    \n",
        "\n",
        "    tokenizer = Tokenizer(num_words=vocab_size, oov_token='<OOV>')\n",
        "\n",
        "    tokenizer.fit_on_texts(train_sentences)\n",
        "    for key, value in tokenizer.word_index.items():\n",
        "        print('{}  \\t======>\\t {}'.format(key, value))\n",
        "        if value == 25:\n",
        "          break\n",
        "    \n",
        "\n",
        "\n",
        "    train_sequences = tokenizer.texts_to_sequences(train_sentences)\n",
        "    validation_sequences = tokenizer.texts_to_sequences(validation_sentences)\n",
        "\n",
        "    train_padded = pad_sequences(train_sequences, maxlen=max_length, truncating=trunc_type, padding=padding_type)\n",
        "    validation_padded = pad_sequences(validation_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "    train_labels = np.array(train_labels)\n",
        "    validation_labels = np.array(validation_labels)\n",
        "\n",
        "\n",
        "    model = tf.keras.Sequential([\n",
        "        \n",
        "        Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "        Bidirectional(LSTM(64, return_sequences=True)),\n",
        "        Bidirectional(LSTM(64)),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dense(16, activation='relu'),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "\n",
        "    checkpoint_path = 'my_checkpoint.ckpt'\n",
        "    checkpoint = ModelCheckpoint(checkpoint_path, \n",
        "                             save_weights_only=True, \n",
        "                             save_best_only=True, \n",
        "                             monitor='val_loss',\n",
        "                             verbose=1)\n",
        "    \n",
        "\n",
        "    epochs=10\n",
        "    history = model.fit(train_padded, train_labels, \n",
        "                        validation_data=(validation_padded, validation_labels),\n",
        "                        callbacks=[checkpoint],\n",
        "                        epochs=epochs)\n",
        "    \n",
        "    model.load_weights(checkpoint_path)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    return model\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    model = solution_model()\n",
        "    model.save(\"TF4-sarcasm.h5\")"
      ],
      "metadata": {
        "id": "67zzNXzKIO-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "뉴럴네트워크"
      ],
      "metadata": {
        "id": "fJNC-zSEJGTj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import urllib\n",
        "import os\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Conv1D, LSTM, Bidirectional\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "def download_and_extract_data():\n",
        "    url = 'https://storage.googleapis.com/download.tensorflow.org/data/certificate/household_power.zip'\n",
        "    urllib.request.urlretrieve(url, 'household_power.zip')\n",
        "    with zipfile.ZipFile('household_power.zip', 'r') as zip_ref:\n",
        "        zip_ref.extractall()\n",
        "\n",
        "\n",
        "# This function normalizes the dataset using min max scaling.\n",
        "# DO NOT CHANGE THIS CODE\n",
        "def normalize_series(data, min, max):\n",
        "    data = data - min\n",
        "    data = data / max\n",
        "    return data\n",
        "\n",
        "def windowed_dataset(series, batch_size, n_past=24, n_future=24, shift=1):\n",
        "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
        "    ds = ds.window(size= (n_past + n_future), # YOUR CODE HERE,\n",
        "                   shift = shift, drop_remainder = True)\n",
        "    # This line converts the windowed dataset into a tensorflow dataset.\n",
        "    ds = ds.flat_map(lambda w: w.batch(n_past + n_future))\n",
        "    ds = ds.map(\n",
        "        lambda w: (w[:n_past], w[n_past:])  ######과거 24개는 x 데이터, 미래 24개는 y데이터\n",
        "    )\n",
        "    return ds.batch(batch_size).prefetch(1)\n",
        "\n",
        "def solution_model():\n",
        "    # Downloads and extracts the dataset to the directory that\n",
        "    # contains this file.\n",
        "    download_and_extract_data()\n",
        "    # Reads the dataset from the csv.\n",
        "    df = pd.read_csv('household_power_consumption.csv', sep=',',\n",
        "                     infer_datetime_format=True, index_col='datetime', header=0)\n",
        "    # Number of features in the dataset. We use all features as predictors to\n",
        "    # predict all features at future time steps.\n",
        "    \n",
        "    N_FEATURES = len(df.columns)\n",
        "    # Normalizes the data\n",
        "    data = df.values\n",
        "    split_time = int(len(data) * 0.8)\n",
        "    data = normalize_series(data, data.min(axis=0), data.max(axis=0))\n",
        "    # Splits the data into training and validation sets.\n",
        "    x_train = data[:split_time]\n",
        "    x_valid = data[split_time:]\n",
        "    # DO NOT CHANGE 'BATCH_SIZE' IF YOU ARE USING STATEFUL LSTM/RNN/GRU.\n",
        "    # THE TEST WILL FAIL TO GRADE YOUR SCORE IN SUCH CASES.\n",
        "    # In other cases, it is advised not to change the batch size since it\n",
        "    # might affect your final scores. While setting it to a lower size\n",
        "    # might not do any harm, higher sizes might affect your scores.\n",
        "    BATCH_SIZE = 32  # ADVISED NOT TO CHANGE THIS\n",
        "    # DO NOT CHANGE N_PAST, N_FUTURE, SHIFT. The tests will fail to run\n",
        "    # on the server.\n",
        "    # Number of past time steps based on which future observations should be\n",
        "    # predicted\n",
        "    N_PAST = 24  # DO NOT CHANGE THIS\n",
        "    # Number of future time steps which are to be predicted.\n",
        "    N_FUTURE = 24  # DO NOT CHANGE THIS\n",
        "    # By how many positions the window slides to create a new window\n",
        "    # of observations.\n",
        "    SHIFT = 1  # DO NOT CHANGE THIS\n",
        "    # Code to create windowed train and validation datasets.\n",
        "    # Complete the code in windowed_dataset.\n",
        "    train_set = windowed_dataset(series=x_train, batch_size=BATCH_SIZE,\n",
        "                                 n_past=N_PAST, n_future=N_FUTURE,\n",
        "                                 shift=SHIFT)\n",
        "    valid_set = windowed_dataset(series=x_valid, batch_size=BATCH_SIZE,\n",
        "                                 n_past=N_PAST, n_future=N_FUTURE,\n",
        "                                 shift=SHIFT)\n",
        "    # Code to define your model.\n",
        "    model = tf.keras.models.Sequential([\n",
        "        Conv1D(filters=32,   # 데이터 사이즈가 선스팟 데이터보다 커서 오래 걸림, filter도 32, lstm도 32로 줌\n",
        "                kernel_size=3,\n",
        "                padding=\"causal\",\n",
        "                activation=\"relu\",\n",
        "                input_shape=[N_PAST, 7],   #24개 * 7개가 들어가고  아웃풋이 24*7 데이터가 예측되어서 나옴\n",
        "                ),\n",
        "        Bidirectional(LSTM(32, return_sequences=True)),\n",
        "        Dense(32, activation=\"relu\"),\n",
        "        Dense(16, activation=\"relu\"),\n",
        "        Dense(N_FEATURES)\n",
        "    ])\n",
        "\n",
        "    checkpoint_path='model/my_checkpoint.ckpt'\n",
        "\n",
        "    checkpoint = ModelCheckpoint(checkpoint_path,\n",
        "                             save_weights_only=True,\n",
        "                             save_best_only=True,\n",
        "                             monitor='val_loss', #  컴파일 할 때 mae로 설정할거기대문에 val_loss ok\n",
        "                             verbose=1,\n",
        "                             )\n",
        "    # learning_rate=0.0005, Adam 옵치마이저\n",
        "    optimizer =  tf.keras.optimizers.Adam(learning_rate=0.0005) # sgd 썼더니 학습속도 느려서 adam 씀\n",
        "\n",
        "    model.compile(loss='mae', #문제에서 mae 기준으로 0.55 어쩌고 적혀잇음\n",
        "              optimizer=optimizer,\n",
        "              metrics=[\"mae\"]\n",
        "              )\n",
        "    model.fit(train_set, \n",
        "        validation_data=(valid_set), \n",
        "        epochs=20, \n",
        "        callbacks=[checkpoint], \n",
        "        )\n",
        "    \n",
        "    model.load_weights(checkpoint_path)\n",
        "    return model\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    model = solution_model()\n",
        "    model.save(\"TF5_HEPC_1.h5\")"
      ],
      "metadata": {
        "id": "VDagAJPpIxn2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}